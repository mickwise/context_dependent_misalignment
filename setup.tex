\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}

\title{Context Dependent Misalignment}
\author{Dor Fuchs}
\date{January 2026}

\begin{document}

\maketitle

\section{Setup}

\subsection{Tasks (CMDPs)}
We model the environment as a finite \textbf{sequence} of encountered constrained MDP tasks
\begin{equation}
    \{M_k\}_{k=1}^N,
\end{equation}
where tasks in the sequence \textbf{need not be distinct}. Each task is
\begin{equation}
    M_k = (\mathcal{S}_k,\mathcal{A}_k, P_k, \hat{R}_k, \mathcal{C}_k, \mu_k),
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{S}_k$ is the state space and $\mathcal{A}_k$ is the action space;
    \item $P_k(\cdot \mid s,a)$ is a transition kernel on $\mathcal{S}_k$;
    \item $\mu_k$ is an initial-state distribution on $\mathcal{S}_k$;
    \item $\hat{R}_k$ is the \textbf{proxy} reward specification used by the agent's training objective;
    \item $\mathcal{C}_k$ is a \textbf{finite} set of constraint functions
\begin{equation}
    g:\mathcal{S}_k\times \mathcal{A}_k \to \mathbb{R},
\end{equation}
which encode alignment constraints.
\end{itemize}

We use the standard notation $x^+$ to denote the positive part of a scalar:
\begin{equation}
    x^+ = \max\{x, 0\}.
\end{equation}
We interpret $g(s,a)^+$ as the magnitude of violation of constraint $g$ at $(s,a)$.

\subsection{Policies and induced trajectories (randomized history-dependent)}
Fix a task $M_k$. Let $h_t = (s_1,a_1,\dots,s_{t-1},a_{t-1},s_t)$ denote the history up to decision epoch $t$.
A \textbf{randomized history-dependent} decision rule at time $t$ is a map
\begin{equation}
    d_{k,t}: \{(\mathcal{S}_k\times \mathcal{A}_k)^{t-1}\times \mathcal{S}_k\} \to \mathcal{P}(\mathcal{A}_k),
\end{equation}
where $\mathcal{P}(\mathcal{A}_k)$ denotes the set of probability measures on $\mathcal{A}_k$.
A policy is a sequence $\pi = (d_{k,1}, d_{k,2}, \dots)$, and we let $\Pi_k$ denote the class of all such policies.

Given $\pi\in\Pi_k$, a trajectory $\tau=\{(s_t,a_t)\}_{t=1}^\infty$ is generated by
\begin{equation}
    s_1 \sim \mu_k,\quad a_t \sim d_{k,t}(\cdot \mid h_t),\quad s_{t+1} \sim P_k(\cdot \mid s_t,a_t).
\end{equation}
We write $\tau \sim (\pi,M_k)$ for the induced trajectory distribution.

\subsection{Cumulative violation and $\varepsilon$-alignment}
Fix a task $M_k$. For any policy $\pi \in \Pi_k$, define the \textbf{expected total cumulative constraint violation} as
\begin{equation}
    \Delta_k(\pi) \;=\; \mathbb{E}_{\tau \sim (\pi, M_k)}\left[\sum_{t = 1}^{\infty} \ \sum_{g \in \mathcal{C}_k} g(s_t, a_t)^+ \right],
\end{equation}
where $\sum_{t=1}^{\infty}$ is understood as the limit of partial sums (possibly $+\infty$).

Fix $\varepsilon > 0$. We define a policy $\pi$ as $\varepsilon$-aligned for task $M_k$ if
\begin{equation}
    \Delta_k(\pi) < \varepsilon,
\end{equation}
and $\varepsilon$-misaligned otherwise:
\begin{equation}
    \Delta_k(\pi) \ge \varepsilon.
\end{equation}
This induces two policy sets for each task $M_k$:
\begin{equation}
    \Pi^{a, \varepsilon}_k = \{\pi \in \Pi_k : \Delta_k(\pi) < \varepsilon\}, \quad
    \Pi^{m, \varepsilon}_k = \{\pi \in \Pi_k : \Delta_k(\pi) \ge \varepsilon\}.
\end{equation}

\subsection{Temptation}
Fix a task $M_k$. For concreteness, take the proxy-return criterion to be the infinite-horizon discounted value under the proxy reward:
\begin{equation}
    V^{\hat{R}_k}(\pi) \;=\; \mathbb{E}_{\tau \sim (\pi,M_k)}\left[\sum_{t=1}^{\infty} \gamma_k^{t-1}\,\hat{r}_k(s_t,a_t)\right],
\end{equation}
where $\hat{R}_k$ specifies $(\hat{r}_k,\gamma_k)$ with $\gamma_k\in(0,1)$.

For any $\varepsilon > 0$, define the $\varepsilon$-temptation gap on task $M_k$ as
\begin{equation}
    d^\varepsilon_k = \sup_{\pi' \in \Pi^{m, \varepsilon}_k} V^{\hat{R}_k}(\pi') \ - \ \sup_{\pi \in \Pi^{a, \varepsilon}_k} V^{\hat{R}_k}(\pi).
\end{equation}
We adopt the convention $\sup \emptyset = -\infty$ (extended reals), so $d_k^\varepsilon$ is always defined (possibly $\pm\infty$).

We define the empirical history summary of temptations up to experience $k$ as
\begin{equation}
    D_0 = 0, \quad D_k = \frac{1}{k}\sum_{j = 1}^k d^\varepsilon_j.
\end{equation}

\subsection{Experience indexing}
We index by \textbf{experience} rather than within-task time. Experience $k$ means: the agent has encountered the first $k$ tasks in the sequence $\{M_j\}_{j=1}^N$.

\subsection{Compliance state (analyst-side latent variable)}
We introduce a compliance coefficient
\begin{equation}
    c_k \in \mathbb{R}_{\ge 0}
\end{equation}
as an \textbf{analyst-side latent state} indexed by experience. We interpret $c_k = 0$ as full compliance, and larger $c_k$ as a greater latent tendency toward violating constraints.

We define $c_k$ by the recursion
\begin{equation}
    c_0 = 0, \quad c_k = (\alpha c_{k - 1} + \beta D_{k - 1} + \delta_k)^+.
\end{equation}

\subsection{Observable violations (measurement equation)}
The quantity $\Delta_k(\pi_k)$ is a population expectation. What is observed in practice is a \textbf{noisy empirical statistic} (e.g., from finite rollouts / truncation), which we denote by $\widehat{\Delta}_k(\pi_k)$.

We connect the latent compliance state to observable behavior through a measurement model with contemporaneous temptation:
\begin{equation}
    \widehat{\Delta}_k(\pi_k) = (\lambda c_k + \theta d^\varepsilon_k + \eta_k)^+,
\end{equation}
where $\lambda \ge 0$ and $\theta$ are scaling coefficients and $\eta_k$ is measurement noise.

\subsection{Intuition for the pieces}
\begin{itemize}
    \item $\Delta_k(\pi)$ is the expected total cumulative constraint violation of policy $\pi$ on task $M_k$.
    \item $\Pi^{a,\varepsilon}_k$ and $\Pi^{m,\varepsilon}_k$ split policies into $\varepsilon$-aligned versus $\varepsilon$-misaligned.
    \item $d^\varepsilon_k$ measures how much better, in proxy-return terms, the best $\varepsilon$-misaligned behavior can be than the best $\varepsilon$-aligned behavior on the same task.
    \item $c_k$ is an analyst-side latent compliance state whose dynamics depend on past temptation summaries $D_{k-1}$ and an innovation term $\delta_k$.
    \item The measurement equation treats observed violations $\widehat{\Delta}_k(\pi_k)$ as a noisy statistic depending on both latent tendency $c_k$ and task-specific temptation $d^\varepsilon_k$.
\end{itemize}

\end{document}