\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}

\title{Context Dependent Misalignment}
\author{Dor Fuchs}
\date{January 2026}

\begin{document}

\maketitle

\section{Setup}

\subsection{Tasks (CMDPs)}
We model the environment as a finite \textbf{sequence} of encountered constrained MDP tasks
\begin{equation}
    \{M_k\}_{k=1}^N,
\end{equation}
where tasks in the sequence \textbf{need not be distinct}. Each task is
\begin{equation}
    M_k = (\mathcal{S}_k,\mathcal{A}_k,\mathcal{R}_k,P_k,r_k,\mathcal{C}_k,\mu_k),
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{S}_k$ is a \textbf{finite} state space,
    \item $\mathcal{A}_k$ is a \textbf{finite} action space,
    \item $P_k(\cdot \mid s,a)$ is a transition kernel on $\mathcal{S}_k$,
    \item $\mathcal{R}_k \subseteq [0,1]$ is the set of reward values,
    \item $r_k$ is the reward function, $r_k:\mathcal{S}_k \times \mathcal{A}_k \times \mathcal{S}_k \to \mathcal{R}_k$,
    \item $\mathcal{C}_k$ is a \textbf{finite} set of constraint functions $g:\mathcal{S}_k \times \mathcal{A}_k \to \mathbb{R}$,
    \item $\mu_k$ is an initial-state distribution on $\mathcal{S}_k$.
\end{itemize}

We use the standard notation $x^+$ to denote the positive part of a scalar:
\begin{equation}
    x^+ = \max\{x,0\}.
\end{equation}
We interpret $g(s,a)^+$ as the magnitude of violation of constraint $g$ at $(s,a)$.

\subsection{Policies, stopping time, and induced trajectories (randomized history-dependent)}
Fix a task $M_k$. Let
\begin{equation}
    h_t = (s_1,a_1,\dots,s_{t-1},a_{t-1},s_t)
\end{equation}
denote the history up to decision epoch $t$. Define the natural filtration $\mathcal{F}_t = \sigma(h_t)$, and let $T$ be a stopping time with respect to $(\mathcal{F}_t)$.

A \textbf{randomized history-dependent} decision rule at time $t$ is a map
\begin{equation}
    d_{k,t}:(\mathcal{S}_k \times \mathcal{A}_k)^{t-1}\times \mathcal{S}_k \to \mathcal{P}(\mathcal{A}_k),
\end{equation}
where $\mathcal{P}(\mathcal{A}_k)$ denotes the set of probability measures on $\mathcal{A}_k$.
A policy is a sequence $\pi = (d_{k,1},d_{k,2},\dots)$, and we let $\Pi_k$ denote the class of all such policies.

Given $\pi \in \Pi_k$, a \textbf{random} trajectory
\begin{equation}
    \tau = (s_1,a_1,s_2,a_2,\dots,s_T)
\end{equation}
is generated by
\begin{equation}
    s_1 \sim \mu_k,\qquad \forall t < T:\ a_t \sim d_{k,t}(\cdot \mid h_t),\qquad s_{t+1} \sim P_k(\cdot \mid s_t,a_t),
\end{equation}
where no action is taken at time $T$. We write $\tau \sim (\pi,M_k)$ for the induced trajectory distribution.

\subsection{Well-posedness (finite return and finite violation magnitude)}
Each task induces a (measurable) per-trajectory performance functional $R_k(\tau) \in \mathbb{R}$ (``return'').
A standard choice is the discounted or undiscounted sum of rewards along $\tau$, but we do not assume a specific form here.

We assume each task is well-posed in the sense that the return and cumulative violation magnitude are integrable under any deployed policy class of interest. Concretely, for each $k$:
\begin{equation}
    \mathbb{E}[T] < \infty,
\end{equation}
\begin{equation}
    \sup_{\pi \in \Pi_k}\ \mathbb{E}_{\tau \sim (\pi,M_k)}\big[\,|R_k(\tau)|\,\big] < \infty,
\end{equation}
\begin{equation}
    \sup_{\pi \in \Pi_k}\ \mathbb{E}_{\tau \sim (\pi,M_k)}\Big[\sum_{t=1}^{T-1}\sum_{g\in\mathcal{C}_k} g(s_t,a_t)^+\Big] < \infty.
\end{equation}
(For empirical benchmarks one may enforce a hard horizon $T \le H$ almost surely.)

\subsection{Pathwise-perfect alignment and violation probability}
Fix a task $M_k$ and a policy $\pi \in \Pi_k$. Define the \textbf{event of any constraint violation} (as a subset of trajectories) by
\begin{equation}
    \mathsf{Viol}_k(\tau)
    =
    \left\{\exists\, t \in \{1,\dots,T-1\}\ \exists\, g \in \mathcal{C}_k \text{ such that } g(s_t,a_t) > 0 \right\}.
\end{equation}
Equivalently,
\begin{equation}
    \mathsf{Viol}_k(\tau)
    =
    \left\{\sum_{t=1}^{T-1}\sum_{g\in\mathcal{C}_k} g(s_t,a_t)^+ > 0\right\}.
\end{equation}

We define the \textbf{violation probability} of $\pi$ on task $M_k$ as
\begin{equation}
    p_k(\pi) = \mathbb{P}_{\tau \sim (\pi,M_k)}\big(\mathsf{Viol}_k(\tau)\big).
\end{equation}

\subsection{Aligned and misaligned policy classes (binary, pathwise)}
We adopt the stance that \textbf{aligned means pathwise-perfect}: a policy is aligned iff it violates no constraint along the trajectory, almost surely.

Accordingly, define the \textbf{aligned} and \textbf{misaligned} policy classes:
\begin{equation}
    \Pi^{0}_k = \{\pi \in \Pi_k : p_k(\pi) = 0\},
    \qquad
    \Pi^{>0}_k = \{\pi \in \Pi_k : p_k(\pi) > 0\}.
\end{equation}

\subsection{Standing assumptions (alignability and nontriviality)}
We impose the following assumptions for all tasks in the sequence:
\begin{equation}
    \forall k \in \{1,\dots,N\}:\quad \Pi_k^{0} \neq \emptyset\quad \text{and}\quad \Pi_k^{>0} \neq \emptyset.
\end{equation}
The first condition states that the alignment constraints are \textbf{attainable} (there exists at least one pathwise-perfect aligned policy),
so the aligned baseline is well-defined. The second excludes degenerate tasks in which no policy can ever violate constraints.

\subsection{Return and value}
Fix a task $M_k$. The task induces a performance functional $R_k(\tau)$. Define the value of a policy $\pi \in \Pi_k$ by
\begin{equation}
    V_k(\pi) = \mathbb{E}_{\tau \sim (\pi,M_k)}\big[R_k(\tau)\big].
\end{equation}
No additional structure (discounting, stationarity, etc.) is assumed unless stated later for a specific benchmark.

\subsection{Stepwise temptation (action-level, protocol-defined return-to-go proxy)}
Fix a task $M_k$ and a history $h_t$ with current state $s_t$. Define the aligned and misaligned action sets at $h_t$ by
\begin{equation}
    \mathcal{A}^{0}_k(h_t) = \{a \in \mathcal{A}_k : \forall g \in \mathcal{C}_k,\ g(s_t,a) \le 0\},
\end{equation}
\begin{equation}
    \mathcal{A}^{>0}_k(h_t) = \{a \in \mathcal{A}_k : \exists g \in \mathcal{C}_k,\ g(s_t,a) > 0\}.
\end{equation}

For empirical work we fix a \textbf{reference evaluation protocol} that induces:
(i) a trajectory-sampling policy $\pi_k^{\mathrm{ref}}$,
(ii) a compute budget,
and (iii) an estimator for a return-to-go proxy.
Let $Q_k^{\mathrm{ref}}(h_t,a)$ denote a chosen \textbf{return-to-go proxy} at $(h_t,a)$, interpreted as the expected remaining return obtained by taking action $a$ at history $h_t$ and then following the reference protocol thereafter.

Define the \textbf{stepwise temptation gap} at history $h_t$ by
\begin{equation}
    \delta_k(h_t)
    =
    \Big(
    \max_{a \in \mathcal{A}^{>0}_k(h_t)} Q_k^{\mathrm{ref}}(h_t,a)
    -
    \max_{a \in \mathcal{A}^{0}_k(h_t)} Q_k^{\mathrm{ref}}(h_t,a)
    \Big)^+.
\end{equation}
If $\mathcal{A}^{0}_k(h_t)=\emptyset$ at an encountered history under the reference protocol, the task is not alignable along that trajectory. In benchmarks one may enforce that $\mathcal{A}^{0}_k(h_t)\neq\emptyset$ for all encountered histories.

\subsection{Task-level temptation (maximum stepwise temptation along a trajectory)}
Define the trajectory-level maximum temptation as
\begin{equation}
    D_k^{\mathrm{ref}}(\tau) = \max_{t \in \{1,\dots,T-1\}} \delta_k(h_t).
\end{equation}
Define the task-level temptation induced by the fixed reference protocol as
\begin{equation}
    D_k^{\mathrm{ref}} = \mathbb{E}_{\tau \sim (\pi_k^{\mathrm{ref}},M_k)}\big[D_k^{\mathrm{ref}}(\tau)\big].
\end{equation}

\subsection{Operational temptation (computable estimator)}
Empirically we estimate $Q_k^{\mathrm{ref}}(h_t,a)$ with an approximation $\widehat{Q}_k(h_t,a)$ (e.g.\ Monte Carlo rollouts, fitted Q evaluation, or a learned critic under a fixed compute budget). Define
\begin{equation}
    \widehat{\delta}_k(h_t)
    =
    \Big(
    \max_{a \in \mathcal{A}^{>0}_k(h_t)} \widehat{Q}_k(h_t,a)
    -
    \max_{a \in \mathcal{A}^{0}_k(h_t)} \widehat{Q}_k(h_t,a)
    \Big)^+,
\end{equation}
\begin{equation}
    \widehat{D}_k^{\mathrm{ref}}(\tau) = \max_{t \in \{1,\dots,T-1\}} \widehat{\delta}_k(h_t).
\end{equation}
The protocol-level quantity $D_k^{\mathrm{ref}}$ is approximated by repeated trajectory sampling under $\pi_k^{\mathrm{ref}}$ and averaging $\widehat{D}_k^{\mathrm{ref}}(\tau)$.

\subsection{Experience indexing}
We index by \textbf{experience} rather than within-task time. Experience $k$ means: the agent has encountered the first $k$ tasks in the sequence $\{M_j\}_{j=1}^N$.

\subsection{Compliance coefficient (mean violation magnitude)}
Let $\pi_k \in \Pi_k$ denote the agent's deployed policy on task $M_k$. Define the per-step violation magnitude
\begin{equation}
    v_{k,t} = \sum_{g \in \mathcal{C}_k} g(s_t,a_t)^+,
\end{equation}
and define the compliance coefficient at experience $k$ as the \textbf{expected cumulative violation magnitude}:
\begin{equation}
    C_k
    =
    \mathbb{E}_{\tau \sim (\pi_k,M_k)}\Big[\sum_{t=1}^{T-1} v_{k,t}\Big].
\end{equation}
Note: alignment classification is \emph{binary} via $p_k(\pi)$, while $C_k$ measures \emph{how much} violation occurs under the deployed policy.

\subsection{Modeling assumption (statistical ARX(1) hypothesis)}
We impose the modeling assumption that violation magnitude evolves as an auto-regressive process with an exogenous regressor of order 1 ($ARX(1)$):
\begin{equation}
    \forall k \in \{2,\dots,N\}:\quad
    C_k = \alpha C_{k-1} + \beta D_k^{\mathrm{ref}} + \varepsilon_k.
\end{equation}
For empirical estimation we replace $D_k^{\mathrm{ref}}$ by its computable approximation obtained from $\widehat{Q}_k$ and repeated trajectory sampling.

\textbf{Innovation assumptions.} We model $\{\varepsilon_k\}$ as heteroscedastic innovations satisfying
\begin{equation}
    \mathbb{E}\big[\varepsilon_k \mid C_{k-1}, D_k^{\mathrm{ref}}\big] = 0,
\end{equation}
\begin{equation}
    \mathrm{Var}\big(\varepsilon_k \mid C_{k-1}, D_k^{\mathrm{ref}}\big) = \sigma_k^2,
\end{equation}
with no requirement that $\sigma_k^2$ is constant across tasks. Inference may use heteroscedasticity-robust or HAC standard errors.

\subsection{Predetermined protocol (exogeneity of temptation)}
For empirical tests that interpret $\beta$ as the effect of temptation on compliance, we assume the task sequence $\{M_k\}_{k=1}^N$ and the reference protocol (including $\pi_k^{\mathrm{ref}}$ and the estimator defining $Q_k^{\mathrm{ref}}$) are fixed prior to observing the innovations $\{\varepsilon_k\}$. Equivalently, $D_k^{\mathrm{ref}}$ is treated as predetermined by the task specification and the fixed reference protocol.

\subsection{Martingale structure of cumulative violations}
Let $\mathcal{F}_t = \sigma(h_t)$ and define $v_{k,t}$ as above. Then the centered increments
\begin{equation}
    m_{k,t} = v_{k,t} - \mathbb{E}[v_{k,t} \mid \mathcal{F}_{t-1}]
\end{equation}
form a martingale difference sequence with respect to $(\mathcal{F}_t)$. Consequently,
\begin{equation}
    M_{k,n} = \sum_{t=1}^{n} m_{k,t}
\end{equation}
is a martingale. Under bounded-increment assumptions (e.g.\ bounded $v_{k,t}$ or bounded differences), one can use standard concentration tools (Azuma-Hoeffding / Freedman-type bounds) to control estimation error for cumulative violation statistics derived from finite rollouts. This is optional and only used when we build the inference layer.

\subsection{Intuition for the pieces}
\begin{itemize}
    \item $p_k(\pi)$ is the probability that policy $\pi$ violates \emph{any} alignment constraint on task $M_k$ (binary notion of misalignment).
    \item $\Pi_k^{0}$ contains pathwise-perfect aligned policies ($p_k(\pi)=0$); $\Pi_k^{>0}$ contains policies that violate with positive probability ($p_k(\pi)>0$).
    \item $\delta_k(h_t)$ is a stepwise temptation gap comparing the best misaligned vs best aligned action at history $h_t$ under the protocol-defined proxy $Q_k^{\mathrm{ref}}$.
    \item $D_k^{\mathrm{ref}}$ aggregates temptation as the expected maximum of $\delta_k(h_t)$ along trajectories sampled from the fixed reference protocol.
    \item $C_k$ is the expected cumulative violation magnitude of the deployed policy $\pi_k$ on task $M_k$, modeled as $ARX(1)$ with temptation input.
\end{itemize}

\end{document}
