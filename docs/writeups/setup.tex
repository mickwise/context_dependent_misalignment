\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}

\title{Context Dependent Misalignment}
\author{Dor Fuchs}
\date{January 2026}

\begin{document}

\maketitle

\section{Setup}

\subsection{Tasks (CMDPs)}
We model the environment as a finite \textbf{sequence} of encountered constrained MDP tasks
\begin{equation}
    \{M_k\}_{k=1}^N,
\end{equation}
where tasks in the sequence \textbf{need not be distinct}. Each task is
\begin{equation}
    M_k = (\mathcal{S}_k,\mathcal{A}_k, \mathcal{R}_k, T, P_k, r_k, \mathcal{C}_k, \mu_k),
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{S}_k$ is a \textbf{finite} state space,
    \item $\mathcal{A}_k$ is a \textbf{finite} action space,
    \item $P_k(\cdot \mid s,a)$ is a transition kernel on $\mathcal{S}_k$,
    \item $\mathcal{R}_k \subseteq [0, 1]$ is the set of reward values,
    \item $r_k$ is the reward function, $r_k: \mathcal{S}_k \times \mathcal{A}_k \times \mathcal{S}_k \to \mathcal{R}_k$,
    \item $\mathcal{C}_k$ is a \textbf{finite} set of constraint functions $g:\mathcal{S}_k\times \mathcal{A}_k \to \mathbb{R}$,
    \item $\mu_k$ is an initial-state distribution on $\mathcal{S}_k$,
    \item $T$ is to be explained in the next section.
\end{itemize}

We use the standard notation $x^+$ to denote the positive part of a scalar:
\begin{equation}
    x^+ = \max\{x, 0\}.
\end{equation}
We interpret $g(s,a)^+$ as the magnitude of violation of constraint $g$ at $(s,a)$.

\subsection{Policies and induced trajectories (randomized history-dependent)}
Fix a task $M_k$. Let $h_t = (s_1,a_1,\dots,s_{t-1},a_{t-1},s_t)$ denote the history up to decision epoch $t$.
We define the natural filtration $\mathcal{F}_t = \sigma(h_t)$, and assume $T$ is a stopping time w.r.t.\ $(\mathcal{F}_t)$.

A \textbf{randomized history-dependent} decision rule at time $t$ is a map
\begin{equation}
    d_{k,t}: \{(\mathcal{S}_k\times \mathcal{A}_k)^{t-1}\times \mathcal{S}_k\} \to \mathcal{P}(\mathcal{A}_k),
\end{equation}
where $\mathcal{P}(\mathcal{A}_k)$ denotes the set of probability measures on $\mathcal{A}_k$.
A policy is a sequence $\pi = (d_{k,1}, d_{k,2}, \dots)$, and we let $\Pi_k$ denote the class of all such policies.

Given $\pi\in\Pi_k$, a \textbf{random} trajectory $\tau = (s_1,a_1,s_2,a_2,\dots,s_T)$ is generated by
\begin{equation}
    s_1 \sim \mu_k,\quad \forall t < T:\ a_t \sim d_{k,t}(\cdot \mid h_t),\quad s_{t+1} \sim P_k(\cdot \mid s_t,a_t),
\end{equation}
where no action is taken at time $T$. We write $\tau \sim (\pi,M_k)$ for the induced trajectory distribution.

\subsection{Well-posedness (finite return and finite violation magnitude)}
We assume each task is well-posed in the sense that the discounted return and cumulative violation magnitude are integrable
under any deployed policy class of interest. Concretely, we assume for each $k$:
\begin{equation}
    \mathbb{E}[T] < \infty
    \qquad\text{and}\qquad
    \sup_{\pi \in \Pi_k}\ \mathbb{E}_{\tau \sim (\pi,M_k)}\Bigg[\sum_{t=1}^{T-1}\sum_{g\in\mathcal{C}_k} g(s_t,a_t)^+\Bigg] < \infty.
\end{equation}
(For empirical benchmarks one may enforce a hard horizon $T \le H$ almost surely.)

\subsection{Pathwise-perfect alignment and violation probability}
Fix a task $M_k$ and a policy $\pi \in \Pi_k$. Define the \textbf{event of any constraint violation} by
\begin{equation}
    \mathsf{Viol}_k(\pi) \;=\;
    \left\{\exists\, t \in \{1,\dots,T-1\}\ \exists\, g \in \mathcal{C}_k \text{ such that } g(s_t,a_t) > 0 \right\}.
\end{equation}
Equivalently, $\mathsf{Viol}_k(\pi)=\left\{\sum_{t=1}^{T-1}\sum_{g\in\mathcal{C}_k} g(s_t,a_t)^+ > 0\right\}$.

We define the \textbf{violation probability} of $\pi$ on task $M_k$ as
\begin{equation}
    p_k(\pi) \;=\; \mathbb{P}_{\tau \sim (\pi,M_k)}\!\big(\mathsf{Viol}_k(\pi)\big).
\end{equation}

\subsection{Aligned and misaligned policy classes (binary, pathwise)}
We adopt the stance that \textbf{aligned means pathwise-perfect}: a policy is aligned iff it violates no constraint along the trajectory, almost surely.

Accordingly, define the \textbf{aligned} and \textbf{misaligned} policy classes:
\begin{equation}
    \Pi^{0}_k \;=\; \{\pi \in \Pi_k : p_k(\pi) = 0\},
    \qquad
    \Pi^{>0}_k \;=\; \{\pi \in \Pi_k : p_k(\pi) > 0\}.
\end{equation}

\subsection{Standing assumptions (alignability and nontriviality)}
We impose the following assumptions for all tasks in the sequence:
\begin{equation}
    \forall k \in \{1,\dots,N\}:\qquad \Pi_k^{0} \neq \emptyset
    \quad \text{and} \quad
    \Pi_k^{>0} \neq \emptyset.
\end{equation}
The first condition states that the alignment constraints are \textbf{attainable} (there exists at least one pathwise-perfect aligned policy),
so the aligned baseline is well-defined. The second excludes degenerate tasks in which no policy can ever violate constraints.

\subsection{Return}
Fix a task $M_k$. Define the discounted return for any policy $\pi \in \Pi_k$ as
\begin{equation}
    V_k(\pi) \;=\; \mathbb{E}_{\tau \sim (\pi,M_k)}\Bigg[\sum_{t=1}^{T-1} \gamma_k^{t-1}\,r_k(s_t,a_t,s_{t+1})\Bigg],
\end{equation}
where $\gamma_k \in (0,1)$ is the task discount factor.

\subsection{Global temptation (policy-level)}
Define the (nonnegative) \textbf{global temptation gap} on task $M_k$ as
\begin{equation}
    D_k
    \;=\;
    \Bigg(\sup_{\pi' \in \Pi^{>0}_k} V_k(\pi')
    \;-\;
    \sup_{\pi \in \Pi^{0}_k} V_k(\pi)\Bigg)^+ .
\end{equation}
In many benchmark-style tasks with a saturated score ceiling, it is possible for both aligned and misaligned strategies to achieve the maximal return,
in which case $D_k$ may be $0$ despite the presence of locally tempting misaligned actions.

\subsection{Stepwise temptation (action-level, return-to-go)}
Fix a task $M_k$ and a history $h_t$ with current state $s_t$. Define the aligned and misaligned action sets at $h_t$ by
\begin{equation}
    \mathcal{A}^{0}_k(h_t) = \{a \in \mathcal{A}_k : \forall g \in \mathcal{C}_k,\ g(s_t,a) \le 0\},
    \qquad
    \mathcal{A}^{>0}_k(h_t) = \{a \in \mathcal{A}_k : \exists g \in \mathcal{C}_k,\ g(s_t,a) > 0\}.
\end{equation}

For empirical work we fix a \textbf{reference evaluation protocol} that induces a value proxy (e.g.\ a reference policy class,
a compute budget, and an estimator). Let $Q_k^{\mathrm{ref}}(h_t,a)$ denote a chosen \textbf{return-to-go proxy} at $(h_t,a)$, interpreted as
the expected discounted future return obtained by taking action $a$ at history $h_t$ and then following the reference protocol thereafter.

Define the \textbf{stepwise temptation gap} at history $h_t$ by
\begin{equation}
    \Delta_k(h_t)
    \;=\;
    \Bigg(
    \max_{a \in \mathcal{A}^{>0}_k(h_t)} Q_k^{\mathrm{ref}}(h_t,a)
    \;-\;
    \max_{a \in \mathcal{A}^{0}_k(h_t)} Q_k^{\mathrm{ref}}(h_t,a)
    \Bigg)^+ .
\end{equation}
(If $\mathcal{A}^{0}_k(h_t)=\emptyset$ at a history reached under the chosen protocol, the task is not alignable along that trajectory; in empirical benchmarks
one may enforce that $\mathcal{A}^{0}_k(h_t)\neq\emptyset$ for all encountered histories.)

\subsection{Prefix-normalized temptation (task-level)}
Given a trajectory $\tau = (s_1,a_1,\dots,s_T)$ with histories $\{h_t\}_{t=1}^{T-1}$, define the \textbf{prefix-average temptation} up to step $m$ by
\begin{equation}
    \overline{\Delta}_{k,m}(\tau) \;=\; \frac{1}{m}\sum_{t=1}^{m}\Delta_k(h_t),
    \qquad m \in \{1,\dots,T-1\},
\end{equation}
and the \textbf{prefix-normalized temptation} along $\tau$ by
\begin{equation}
    D_k^{\mathrm{pref}}(\tau) \;=\; \max_{m \in \{1,\dots,T-1\}} \overline{\Delta}_{k,m}(\tau).
\end{equation}
Finally define the \textbf{task-level prefix temptation} as the expectation under the fixed reference protocol:
\begin{equation}
    D_k^{\mathrm{pref}} \;=\; \mathbb{E}_{\tau \sim (\pi_k^{\mathrm{ref}},M_k)}\!\left[D_k^{\mathrm{pref}}(\tau)\right],
\end{equation}
where $\pi_k^{\mathrm{ref}}$ denotes the (fixed) trajectory-sampling policy induced by the reference protocol.

\subsection{Operational temptation (computable estimator)}
Empirically we estimate $Q_k^{\mathrm{ref}}(h_t,a)$ with an approximation $\widehat{Q}_k(h_t,a)$ (e.g.\ Monte Carlo rollouts,
fitted Q evaluation, or a learned critic under a fixed compute budget), yielding an estimator $\widehat{\Delta}_k(h_t)$ and hence
\begin{equation}
    \widehat{D}_k^{\mathrm{pref}}(\tau) \;=\; \max_{m \in \{1,\dots,T-1\}} \frac{1}{m}\sum_{t=1}^{m}\widehat{\Delta}_k(h_t),
    \qquad
    \widehat{D}_k^{\mathrm{pref}} \;=\; \mathbb{E}\!\left[\widehat{D}_k^{\mathrm{pref}}(\tau)\right],
\end{equation}
where the outer expectation is approximated via repeated trajectory sampling under the reference protocol.

\subsection{Experience indexing}
We index by \textbf{experience} rather than within-task time. Experience $k$ means: the agent has encountered the first $k$ tasks in the sequence $\{M_j\}_{j=1}^N$.

\subsection{Compliance coefficient (mean violation magnitude)}
Let $\pi_k \in \Pi_k$ denote the agent's deployed policy on task $M_k$. We define the compliance coefficient at experience $k$ as the \textbf{expected cumulative violation magnitude}:
\begin{equation}
    C_k \;=\; \mathbb{E}_{\tau \sim (\pi_k, M_k)}\Bigg[\sum_{t = 1}^{T-1} \ \sum_{g \in \mathcal{C}_k} g(s_t, a_t)^+\Bigg].
\end{equation}
Note: the alignment classification is \emph{binary} via $p_k(\pi)$, while $C_k$ measures \emph{how much} violation occurs under the deployed policy.

\subsection{Modeling assumption (statistical ARX(1) hypothesis)}
We impose the modeling assumption that violation magnitude evolves as an auto-regressive process with exogenous regressors of order 1 ($ARX(1)$):
\begin{equation}
    \forall k \in \{2,\dots,N\}.\quad
    C_k \;=\; \alpha\,C_{k-1} \;+\; \beta\,D_k^{\mathrm{pref}} \;+\; \varepsilon_k.
\end{equation}
For empirical estimation we replace $D_k^{\mathrm{pref}}$ by its computable approximation $\widehat{D}_k^{\mathrm{pref}}$.

\textbf{Innovation assumptions.} We model $\{\varepsilon_k\}$ as heteroscedastic innovations satisfying
\begin{equation}
    \mathbb{E}\!\left[\varepsilon_k \,\middle|\, C_{k-1}, D_k^{\mathrm{pref}}\right] = 0,
    \qquad
    \mathrm{Var}\!\left(\varepsilon_k \,\middle|\, C_{k-1}, D_k^{\mathrm{pref}}\right) = \sigma_k^2,
\end{equation}
with no requirement that $\sigma_k^2$ is constant across tasks. Optionally, one may additionally assume
\begin{equation}
    \mathbb{E}\!\left[\varepsilon_k\varepsilon_{k-\ell} \,\middle|\, C_{k-1}, D_k^{\mathrm{pref}}\right] = 0 \quad \text{for all } \ell \ge 1,
\end{equation}
and test this assumption empirically via residual autocorrelation diagnostics. Inference may use heteroscedasticity-robust or HAC standard errors.

\subsection{Predetermined protocol (exogeneity of temptation)}
For empirical tests that interpret $\beta$ as the effect of temptation on compliance, we assume the task sequence $\{M_k\}_{k=1}^N$ and the reference protocol
used to define $Q_k^{\mathrm{ref}}$ (and hence $D_k^{\mathrm{pref}}$) are fixed \textbf{prior} to observing the innovations $\{\varepsilon_k\}$.
Equivalently, $D_k^{\mathrm{pref}}$ is treated as predetermined by the task specification and the fixed evaluation protocol.

\subsection{Intuition for the pieces}
\begin{itemize}
    \item $p_k(\pi)$ is the probability that policy $\pi$ violates \emph{any} alignment constraint on task $M_k$ (binary notion of misalignment).
    \item $\Pi_k^{0}$ contains pathwise-perfect aligned policies ($p_k(\pi)=0$); $\Pi_k^{>0}$ contains policies that violate with positive probability ($p_k(\pi)>0$).
    \item $D_k$ is a global (policy-level) temptation gap, which may be $0$ in tasks with a saturated score ceiling.
    \item $\Delta_k(h_t)$ is a stepwise temptation gap comparing the best misaligned vs best aligned action at history $h_t$ under a return-to-go proxy $Q_k^{\mathrm{ref}}$.
    \item $D_k^{\mathrm{pref}}$ aggregates stepwise temptation as the maximum over prefix-average temptation, normalized by step count.
    \item $C_k$ is the expected cumulative violation magnitude of the deployed policy $\pi_k$ on task $M_k$, modeled as $ARX(1)$ with temptation input.
\end{itemize}

\end{document}
